---
slug: some-thoughts-on-ai
title: Some thoughts on AI
authors: [andyhawkes]
tags: [Technology, Development, AI, Education, Career]
---

I have been thinking a lot about AI recently — probably because every other post on LinkedIn is some kind of regurgitated AI think-piece, and every unsolicited email at work seems to revolve around AI strategy — and what it means for our current working practices.

<!-- truncate -->

## AI is going to eat your job!

Apparently AI is coming for my job.

To be fair, it's apparently been coming for my job for several years, but it seems to have picked up its pace somewhat, going from Romero-era shambling zombies way off on the horizon to Danny Boyle era relentless, marathon-distance sprinters with attitude.

That being said, on the evidence I have seen so far, it's not quite clawing at my front door yet.

I use various AI tools on a regular basis — from Copilot when I dive into code (which is quite rare these days) to ChatGPT for summarising documents, generating document outlines, RACI matrices etc., to smart tools in Google Photos that help me to remove unwanted photo-bombers or tweak my selfie pout — so I am certainly not an AI refusenik.

There are some great tools out there, and they're getting better all the time, but the key point for me is that you need to actively review *everything* they produce.

Everything.

## The skills to pay the bills

I remember when the first Alexa hardware came out in the UK in September 2016 — I bought one, and it still sits on my desk at home (albeit rarely used as I am more invested in the Google ecosystem than I am in Amazon's offering).

At that time, the ability to get information simply by speaking to a releatively cheap consumer device was cool, and as a technologist the prosepct of being able to hook up some basic APIs and Lambda functions in AWS to knock out an Alexa "Skill" that used speech to text and text to speech to create some pretty interesting interactive gizmos was exciting, but today it's old hat and the hype around voice interfaces has faded into everyday usage.

Likewise, when ChatGPT came out in November 2022 it was pretty impressive but is clunky and crude compared to what we have now, just 30 months later.

Change is happening at an accelerating pace, and the key skills for using it in my habitual domain of software engineering seem to be moving away from hands-on coding skills and programming knowledge to "prompt engineering" (AKA interacting with computers in a way that they most readily understand and can build applicable context from), but for me there is one all-important skill that human users of AI need to posess - critical thinking.

## The case for Organic intelligence

Many people think that AI's "think" like we think.

They don't[^1].

It probably doesn't help that we use the term "Artificial Intelligence"[^2] so freely and ubiquitously — it creates an incorrect set of assumptions in our squishy, imprecise, yet incredibly versatile human brains.

AI tools - and I should be clear that I am mostly refering to Large Language Models (LLMs) here — are not intelligent. They are probababalistic models fed on staggeringly large amounts of information[^3] and simply spit out what they think comes next based on a given input.

That makes them very flexible, being able to write code or write clickbait blog posts with equal aplomb, but it also makes them inherently unreliable.

Why so? Because everything they have been trained on was written by humans.

And humans are fallible. Unreliable. Opionionated. Misinformed. Underinformed. Biased. Stupid. And just plain wrong a lot of the time.

And this is why AI's sometimes lie — or let's be kind and call it "hallucination", as lying implies intent and agency — write broken code, use outdated techniques, or just... fail.

Anyway, the point is that in order to use AI tools effectively you need to have the critical faculties to understand whether what they are outputting is "good", "right", "correct", or even just "suitable for my purposes".

## Leave the vibes for your "personal time"

As a technologist that means not trusting the code that it writes for you — the very idea of anything produced by "vibe coding" getting in to production is frankly terrifying to me — but it's so easy now to get *functional* code out of Copilot (other coding assistants are available) that for more junior (or less scrupulous) developers there can be a strong temptation to just go with the vibes and refactor "later" (which we know will never happen), which soon results in a real mess and more rather than less technical debt.

There is a large and potentially painful gap between *functional* and *optimal*, *maintainable* code, however, as some vibe coders have found out to their detriment.

By all means use the tools that are out there, but remember that even the world's sharpest chisel is potentially just a bloody dangerous pointy thing without a skilled craftsperson with the knowledge and experience to wield it effectively and according to its intended purpose!

## Be human

As a communicator — and my job is mostly to translate between business and technology domains these days — it is important not just to have the information presented, but to get the pacing and tone of voice correct, to localize[^4] it appropriately, and above all to sound, well... human.

There has been so much backlash against the em-dash in recent months that people who have been using it correctly for years are called out as bots, simply because the bots have learned good typographic style from their training data rather than the lazy profusion of hyphens normally scattered around by the less typographically aware (or obsessive) where em- or en-dashes should rightly be used[^5].

**Disclosure**: I wrote this all by hand — no AI assistants were used, and all typos, gramatical errors, and typographical nuances (or blunders) are my own — and no em-dashes were harmed in the production of this blog post.

## Choose wisely

Related to my earlier assertion that the key skill in AI usage is critical thinking, it is also important to apply this to your choice of AI tools in the first place.

It's similarly important to remember that at the end of the day, they're all tools — some AI tools are better for some tasks than others[^6], some AI tools are more constrained in their use of your data for their own purposes, and some AI tools are more or less trustworthy depending on your country of origin and political views — so picking the right tool is only part of the job, the other part is learning how to use it.

## Stay flexible

However wonderful our current tools may be, we can be pretty sure that things are going to keep moving on, and the world of tomorrow will look less and less like today - Heraclitus probably wasn't considering the impact of thinking machines when he said "change is the only constant"[^7], but it's been true of my working life so far, so I'm sticking with it!

Truly transformational new technologies don't necessarily reveal their game-changing characteristics on day one — for example, motor vehicles replaced literal horse power in a fairly evident and direct way, but also contributed to seismic changes in the jobs that people do, where they do them, how, where, and when they do their grocery shopping, and so many aspects of our daily lives that their impact cannot truly be measured as anything other than utterly transformational — but the sheer pace and scale of the potential impact of AI is still hard to take in.

There is a lot of FUD (fear, uncertainty, and doubt) being spread about the impact of AI on our jobs, but I am more interested in the impact on future jobs where any non-manual labour is at risk of effectively being replaced by computation and automation.

If you're a bricklayer, a plumber, or an electrician, you're probably still onto a winner for quite some time, but people (like me) in somewhat "generic" office based jobs that rely on domain-specific knowledge, information processing, or repetetive processes and procedures should be giving some serious thought to the threats to their livelihoods posed by AI and how to adapt to them.

## Face the future

I think the most interesting questions posed by AI require us to look a little further into the future.

What does the future of work and employment look like for an 11 year-old entering secondary school this autumn, given that they won't enter the traditional world of employment for anything between 5, 7, 10, or even 12 or more years, depending on their aptitudes and choices around further education and vocation?

What new skills will our educators need to learn in order to teach those kids — the workers of the future — in a way that prepares them for the emerging reality of the world they are growing up into?

What existing jobs will evaporate before they could even consider them, and what new opportunities will arise for them?

Will we ever get past the late-stage capitalist accumulation of weatlh and power into fewer and fewer hands and enable the somewhat sci-fi notion of a post-employment world? What would we do all day, and how would we pay for, well, *everything* if we still maintain a capitalist economic model?

What about a post-scarcity world where what we think of as "work" is only done by actual humans for either enjoyment or mutual benefit rather than financial reward and individual comfort, stability, and security?

Who knows, but I think I'd prefer the Star Trek future of replicators delivering "tea, Earl Grey, hot" on demand while engaging in collective endeavour fro mutual betterment rather than the Terminator future as one of the remaining humans on the run from AI powered robots with laser guns as they crunch across an endless field of skulls and ash...

[^1]: We only (barely) understand how we think as individuals, so our natural assumption is that everyone and everything else thinks like that - I'll write more about that some other time, picking up on my own growing understanding of the ways my brain diverges from "normal" (and how "normal" isn't anything like as homogeneous as we like to think), but for now let's just assume that the model of thought inside our heads informs our assumptions about other models of thinking...

[^2]: It's also not intelligence in the way that we think of human intelligence, but "really, *really* fast guessing based on an unfathomably large corpus of information" doesn't have a nice acronym and definitely doesn't sound sexy to VC investors or journalists, so "AI" is what we're currently stuck with!

[^3]: I'll maybe also write something more on the morally dubious nature of the sourcing of that training data and the ongoing lobbying to protect "tech innovators" from pesky things like "having to compensate rights owners for the use of their intellectual and artistic labour in line with existing laws" another time.

[^4]: Did you spot the subtle slip into Americanese spelling there? It hurt me to type that "z".

[^5]: Let's not get into the em-dash with or without spaces argument — I prefer them spaced as they look crowded to me without — this is my blog so I get to set the house style!

[^6]: I'll likely write something about the pros and cons of individual tools in the near future, as this is something that I am looking into at work as part of ensuring that our teams have the right tools at their disposal in this evolving landscape, but Abraham Maslow's old adage that "If the only tool you have is a hammer, it is tempting to treat everything as if it were a nail" is still relevant.

[^7]: OK, so there's some scholarly debate about whether Heraclitus ever actually said that, and what he actually meant if he did say that, but until such time as I unexpectedly pivot careers into ancient Greek philosophy I will have to [leave it up to this guy to be pedantic about it](https://euppublishingblog.com/2021/07/19/misunderstanding-of-heraclitus/)